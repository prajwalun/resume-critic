Tracing
Introduction
Learn how to use all features of Judgment's real-time tracing module.
Quickstarts
Tracing
To get started with tracing, you can decorate your functions with the @judgment.observe() decorator.
For LLM API clients, you can capture generation telemetry using the wrap() function.
Any function decorated with @judgment.observe() will be traced as its own span.
tracing.py
from judgeval.tracer import Tracer, wrapfrom openai import OpenAIclient = wrap(OpenAI())  # tracks all LLM callsjudgment = Tracer(project_name="my_project")@judgment.observe(span_type="tool")def format_question(question: str) -> str:    # dummy tool    return f"Question : {question}"@judgment.observe(span_type="function")def run_agent(prompt: str) -> str:    task = format_question(prompt)    response = client.chat.completions.create(        model="gpt-4.1",        messages=[{"role": "user", "content": task}]    )    return response.choices[0].message.contentrun_agent("What is the capital of the United States?")
You will see your trace unfold live in the UI, with each span and event appearing in real time as the operation executes. Check your terminal for a link to view the trace!
Integrate With Online evals
You can run evals on your traces with any of judgeval's built-in scorers in real-time, enabling you to flag and alert on regressions in production.
To run an online eval, it takes one line of code with the async_evaluate() function. In this example, we'll use the AnswerRelevancyScorer to evaluate the relevance of the agent's response to the user's query.
tracing.py
from judgeval.common.tracer import Tracer, wrapfrom judgeval.scorers import AnswerRelevancyScorerfrom openai import OpenAIclient = wrap(OpenAI())judgment = Tracer(project_name="my_project")@judgment.observe(span_type="tool")def format_question(question: str) -> str:    # dummy tool    return f"Question : {question}"@judgment.observe(span_type="function")def run_agent(prompt: str) -> str:    task = format_question(prompt)    response = client.chat.completions.create(        model="gpt-4.1",        messages=[{"role": "user", "content": task}]    )    answer = response.choices[0].message.content    judgment.async_evaluate(        scorers=[AnswerRelevancyScorer(threshold=0.5)],        input=task,        actual_output=answer,        model="gpt-4.1"    )    print("Online evaluation submitted.")    return answerrun_agent("What is the capital of the United States?")
You should see the online eval results on the Judgment platform shortly after the trace is recorded. Evals can take time to execute, so they may appear slightly delayed on the UI. Once the eval is complete, you should see it attached to your trace like this:
 Trace with online eval 

Trace Your Agents
When working with multi-agent systems, it can be useful to see very easily which agents were calling methods throughout a trace.
In order to aid with this, you can also decorate a class with the @judgment.identify() decorator.
Within the decorator, you can specify what class attribute will be used as the identifier for each agent with the identifier parameter.
trace_agents.py
from judgeval.common.tracer import Tracerjudgment = Tracer(project_name="multi_agent_system")# The judgment.identify() specifies that the agent's will be # identified based on their "name" attribute.@judgment.identify(identifier="name")class SimpleAgent:    def __init__(self, name: str):        self.name = name           @judgment.observe(span_type="tool")    def send_message(self, content: str) -> None:        return f"Message sent with content: {content}"@judgment.observe(span_type="function")def main():    alice = SimpleAgent("Alice")  # agent will be identified as "Alice"    bob = SimpleAgent("Bob")    alice.send_message("Hello Bob, how are you?")    bob.send_message("I'm good Alice, thanks for asking!")main()   
The trace should show up in the Judgment platform clearly indicating which agent called which method (using square brackets):
 Trace with agent names 

After running this trace, you can export the complete agent environment data from the Judgment platform:
1. Navigate to your trace in the platform
2. Click the "Fetch Tool Calls" button in the trace view
   * All tool calls by agent: Which specific agent made each tool call with full attribution
   * Input/environment reactions: How each agent responded to environmental inputs and state changes
   * Trajectories of entire trace: Complete execution path showing agent decision flows and interaction patterns
   * Added metadata: All custom metadata set via judgment.set_metadata() calls
The exported data will include comprehensive information about each agent's statea and behavior, making it easy to analyze multi-agent interactions, debug complex scenarios, and optimize agents for your environment.
Toggling Monitoring
If your setup requires you to toggle monitoring in production-level environments, you can disable monitoring by:
* Setting the JUDGMENT_MONITORING environment variable to false (Disables tracing)
export JUDGMENT_MONITORING=false
* Setting the JUDGMENT_EVALUATIONS environment variable to false (Disables async_evaluates)
export JUDGMENT_EVALUATIONS=false




Tracing
Saving & Exporting Traces
Learn how to save and export traces to external platforms for fine tuning and long-term storage.
If you'd like to save and export traces to an external platform we don't support, we recommend opening a feature request on our GitHub.
Save to S3
The Tracer supports saving traces to Amazon S3 buckets for persistent storage and analysis. This is particularly useful for long-term storage of traces and for sharing traces across different environments.
Configuration
To enable S3 storage for your traces, you'll need to configure the Tracer with your AWS credentials and bucket information:
export_traces.py
judgment = Tracer(    project_name="my_project",    use_s3=True,    s3_bucket_name="my-traces-bucket",  # Bucket will be created automatically if it doesn't exist    s3_aws_access_key_id="your-access-key",  # Optional    s3_aws_secret_access_key="your-secret-key",  # Optional    s3_region_name="us-west-1"  # Optional)
s3_aws_access_key_id, s3_aws_secret_access_key, and s3_region_name are optional. If they are not provided, your AWS credentials will be searched for in the following order:
1. Environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION
2. Configuration file: ~/.aws/credentials
Trace Management in S3
* Traces will be stored in the following folder structure: {bucket_name}/traces/{s3_bucket_name}/{trace_id}_{timestamp}.json
* The specified S3 bucket will be created automatically if it doesn't exist
* Make sure your AWS credentials have sufficient permissions to:
   * Create buckets (if the bucket doesn't exist)
   * Write objects to the bucket
   * List objects in the bucket
   * Read objects from the bucket
* Traces stored in S3 can be accessed using standard AWS tools, such as the AWS CLI or the AWS Management Console. Each trace is stored as a separate object in the bucket, with a unique identifier and timestamp.
For the folder structure of the saved traces, timestamp refers to the UTC timestamp of when the trace was saved in format YYYYMMDD_HHMMSS.
Example Usage
from judgment import Tracertracer = Tracer(    project_name="my_project",    use_s3=True,    s3_bucket_name="my-traces-bucket")# Use the tracer as normal - traces will be automatically saved to S3@judgment.observe(span_type="function")def my_workflow():    print("Hello world!")



Tracing/Tracing Integrations
LangGraph
Integrating Judgeval with LangGraph allows for detailed tracing and evaluation of your graph workflows. By adding the JudgevalCallbackHandler to your LangGraph invocation, you can automatically trace node executions, tool calls, and LLM interactions within your graph using Judgeval.
We expect you to already be familiar with Judgeval and its core concepts. If not, please refer to the quickstarts guide.
Judgeval Callback Handler
Judgeval provides the JudgevalCallbackHandler for LangGraph integration, which works with both synchronous (graph.invoke) and asynchronous (graph.ainvoke) LangGraph workflows.
from judgeval.common.tracer import Tracerfrom judgeval.integrations.langgraph import JudgevalCallbackHandlerjudgment = Tracer(project_name="my-project")handler = JudgevalCallbackHandler(judgment)
The handler automatically captures:
Visits to each node in your graph.
Calls made to any tools integrated with your LangGraph agents or nodes.
LLM generations



Synchronous agent example 


from typing import TypedDict, Sequence
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from judgeval.common.tracer import Tracer
from judgeval.integrations.langgraph import JudgevalCallbackHandler




class State(TypedDict):
    messages: Sequence[HumanMessage]
    # ...


judgment = Tracer(project_name="my-project")
handler = JudgevalCallbackHandler(judgment)


def node_1(state: State):
    # ... node logic ...
    # Optionally add evaluation here using add_evaluation_to_state(state, ...)
    return state


def node_2(state: State):
    # ... node logic ...
    return state


graph_builder = StateGraph(State)
graph_builder.add_node("node_1", node_1)
graph_builder.add_node("node_2", node_2)
graph_builder.set_entry_point("node_1")
graph_builder.add_edge("node_1", "node_2")
graph_builder.add_edge("node_2", END)
graph = graph_builder.compile()


def run_graph():
    initial_state = {"messages": [HumanMessage(content="Hello!")]}
    config_with_callbacks = {"callbacks": [handler]}
    
    final_state = graph.invoke(initial_state, config=config_with_callbacks)


    print("Executed Nodes:", handler.executed_nodes)
    print("Executed Tools:", handler.executed_tools)
    print("Node/Tool Flow:", handler.executed_node_tools)


    print("Final State:", final_state)


if __name__ == "__main__":
    run_graph()

Async example

import asyncio
from typing import TypedDict, Sequence
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from judgeval.common.tracer import Tracer
from judgeval.integrations.langgraph import JudgevalCallbackHandler


class State(TypedDict):
    messages: Sequence[HumanMessage]
    # ...


judgment = Tracer(project_name="my-project")
handler = JudgevalCallbackHandler(judgment)


async def node_1(state: State):
    # ... node logic ...
    # Optionally add evaluation here using add_evaluation_to_state(state, ...)
    return state


async def node_2(state: State):
    # ... node logic ...
    return state


graph_builder = StateGraph(State)
graph_builder.add_node("node_1", node_1)
graph_builder.add_node("node_2", node_2)
graph_builder.set_entry_point("node_1")
graph_builder.add_edge("node_1", "node_2")
graph_builder.add_edge("node_2", END)
graph = graph_builder.compile()


async def run_graph():
    initial_state = {"messages": [HumanMessage(content="Hello!")]}
    config_with_callbacks = {"callbacks": [handler]}
    
    final_state = await graph.ainvoke(initial_state, config=config_with_callbacks)


    print("Executed Nodes:", handler.executed_nodes)
    print("Executed Tools:", handler.executed_tools)
    print("Node/Tool Flow:", handler.executed_node_tools)


    print("Final State:", final_state)


if __name__ == "__main__":
    asyncio.run(run_graph())



The handler instance stores metadata about the execution, accessible after the graph run:
executed_nodes: A list of node names that were executed.
executed_tools: A list of tool names that were called.
executed_node_tools: A list detailing the sequence of node and tool executions, formatted like ['node_A', 'node_A:tool_X', 'node_B'].
print(handler.executed_nodes, handler.executed_tools, handler.executed_node_tools)
When using the callback handler, you do not need to add @observe decorators to your LangGraph nodes or tools for tracing purposes.
Triggering Evaluations
You can trigger Judgeval evaluations directly from within your graph nodes. This associates the evaluation results with the specific node's execution span in the trace. The recommended way is to use the add_evaluation_to_state helper function:



Langgraph_agent.py

# Inside your LangGraph node function
from judgeval.integrations.langgraph import add_evaluation_to_state
from judgeval.scorers import AnswerRelevancyScorer # Or other scorers


def my_node_function(state: State) -> State:
    # ... your node logic ...
    user_input = "some input"
    llm_output = "some output"
    model_name = "gpt-4"


    add_evaluation_to_state(
        state=state,
        scorers=[AnswerRelevancyScorer(threshold=0.7)],
        input=user_input,
        actual_output=llm_output,
        model=model_name
    )
    # ... potentially modify state further ...
    return state


Async

# Inside your LangGraph node function
from judgeval.integrations.langgraph import add_evaluation_to_state
from judgeval.scorers import AnswerRelevancyScorer # Or other scorers


async def my_async_node_function(state: State) -> State:
    # ... your node logic ...
    user_input = "some input"
    llm_output = "some output"
    model_name = "gpt-4"


    add_evaluation_to_state(
        state=state,
        scorers=[AnswerRelevancyScorer(threshold=0.7)],
        input=user_input,
        actual_output=llm_output,
        model=model_name
    )
    # ... potentially modify state further ...
    return state



Alternatively, you can manually add an EvaluationConfig object to your node's output state under the key _judgeval_eval. The handler will detect this and trigger the evaluation.