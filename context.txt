judgment-cookbook
This repo contains cookbooks demonstrating evaluations of AI Agents using the judgeval package implemented by Judgment Labs.
Prerequisites
Before running these examples, make sure you have:
1. Installed the latest version of the Judgeval package:
2. pip install judgeval
3. Set up your Judgeval API key and organization ID as environment variables:
export JUDGMENT_API_KEY="your_api_key"
4. export JUDGMENT_ORG_ID="your_org_id"
To get your API key and Organization ID, make an account on the Judgment Labs platform.
Cookbooks Overview
This repository provides a collection of cookbooks to demonstrate various evaluation techniques and agent implementations using Judgeval.
Handrolled API Agent Examples
These cookbooks feature agents that interact directly with LLM APIs (e.g., OpenAI, Anthropic), often implementing custom logic for tool use, function calling, and RAG.
* multi-agent/: A flexible multi-agent framework for orchestrating and evaluating the collaboration of multiple agents and tools on complex tasks like financial analysis. Evaluated on factual adherence to retrieval context.
LangGraph Agent Examples
These cookbooks showcase agents built using the LangGraph framework, demonstrating complex state management and chained operations.
* langgraph_music_recommender/: An agent that generates song recommendations based on user music taste.
Writing Custom Scorers
These cookbooks focus on how to implement and use custom scorers:
* custom_scorers/: Provides examples of how to implement and use custom scorers to tailor evaluations to specific needs beyond built-in scorers.





Music Recommendation Agent (LangGraph)
A basic LangGraph agent that generates music recommendations based on user taste.
Uses the TavilySearch API to perform web searching over artist music.
Running the agent
The agent is configured in music_agent.py to run, along with triggering a unit test on its tool-calling order (expected to fail).
If you want to run the agent without the evaluation, uncomment the section in the if __name__ == "__main__": section.
Integration Points
Integration Point
	Where/How Used
	JudgmentClient
	For running and logging evaluations/assertions
	Tracer
	For tracing and monitoring the workflow
	JudgevalCallbackHandler
	For integrating tracing with LangGraph execution
	async_evaluate
	For online evaluation of model outputs during workflow
	Scorers
	For specifying evaluation criteria (relevancy, tool order, etc.)
	Example/Assertions
	For defining and running test cases with expected tool usage
	

import os
from typing import TypedDict, Sequence, Dict, Any, Optional, List, Union
from openai import OpenAI
from dotenv import load_dotenv
from tavily import TavilyClient
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from judgeval.common.tracer import Tracer
from judgeval.integrations.langgraph import JudgevalCallbackHandler
from judgeval.scorers import AnswerRelevancyScorer, ToolOrderScorer
from judgeval import JudgmentClient
from judgeval.data import Example


# Load environment variables
load_dotenv()


client = JudgmentClient()
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
chat_model = ChatOpenAI(model="gpt-4", temperature=0)


judgment = Tracer(
    api_key=os.getenv("JUDGMENT_API_KEY"),
    project_name="music-recommendation-bot",
    enable_monitoring=True,  # Explicitly enable monitoring
    deep_tracing=False # Disable deep tracing when using LangGraph handler
)


# Define the state type
class State(TypedDict):
    messages: Sequence[HumanMessage | AIMessage]
    preferences: Dict[str, str]
    search_results: Dict[str, Any]
    recommendations: str
    current_question_idx: int
    questions: Sequence[str]


# Node functions
def initialize_state() -> State:
    """Initialize the state with questions and predefined answers."""
    questions = [
        "What are some of your favorite artists or bands?",
        "What genres of music do you enjoy the most?",
        "Do you have any favorite songs currently?",
        "Are there any moods or themes you're looking for in new music?",
        "Do you prefer newer releases or classic songs?"
    ]
    
    # Predefined answers for testing
    answers = [
        "Taylor Swift, The Beatles, and Ed Sheeran",
        "Pop, Rock, and Folk",
        "Anti-Hero, Hey Jude, and Perfect",
        "Upbeat and energetic music for workouts",
        "I enjoy both new and classic songs"
    ]
    
    # Initialize messages with questions and answers alternating
    messages = []
    for question, answer in zip(questions, answers):
        messages.append(HumanMessage(content=question))
        messages.append(AIMessage(content=answer))
    
    return {
        "messages": messages,
        "preferences": {},
        "search_results": {},
        "recommendations": "",
        "current_question_idx": 0,
        "questions": questions
    }


def ask_question(state: State) -> State:
    """Process the next question-answer pair."""
    if state["current_question_idx"] >= len(state["questions"]):
        return state
    
    # The question is already in messages, just return the state
    return state


def process_answer(state: State) -> State:
    """Process the predefined answer and store it in preferences."""
    messages = state["messages"]
    
    # Ensure we have both a question and an answer
    if len(messages) < 2 or state["current_question_idx"] >= len(state["questions"]):
        return state
    
    try:
        last_question = state["questions"][state["current_question_idx"]]
        # Get the answer from messages - it will be after the question
        answer_idx = (state["current_question_idx"] * 2) + 1  # Calculate the index of the answer
        last_answer = messages[answer_idx].content
        
        state["preferences"][last_question] = last_answer
        state["current_question_idx"] += 1
        
        # Print the Q&A for visibility
        print(f"\nQ: {last_question}")
        print(f"A: {last_answer}\n")
        
    except IndexError:
        return state
    
    return state


def search_music_info(state: State) -> State:
    """Search for music recommendations based on preferences."""
    preferences = state["preferences"]
    search_results = {}
    
    # Search for artist recommendations
    if preferences.get("What are some of your favorite artists or bands?"):
        artists_query = f"Music similar to {preferences['What are some of your favorite artists or bands?']}"
        search_results["artist_based"] = tavily_client.search(
            query=artists_query,
            search_depth="advanced",
            max_results=5
        )
    
    # Search for genre recommendations
    if preferences.get("What genres of music do you enjoy the most?"):
        genre_query = f"Best {preferences['What genres of music do you enjoy the most?']} songs"
        search_results["genre_based"] = tavily_client.search(
            query=genre_query,
            search_depth="advanced",
            max_results=5
        )
    
    # Search for mood-based recommendations
    mood_question = "Are there any moods or themes you're looking for in new music?"  # Fixed apostrophe
    if preferences.get(mood_question):
        mood_query = f"{preferences[mood_question]} music recommendations"
        search_results["mood_based"] = tavily_client.search(
            query=mood_query,
            search_depth="advanced",
            max_results=5
        )
    
    state["search_results"] = search_results
    return state


def generate_recommendations(state: State) -> State:
    """Generate personalized music recommendations using ChatOpenAI."""
    preferences = state["preferences"]
    search_results = state["search_results"]
    
    # Prepare context from search results
    context = ""
    for category, results in search_results.items():
        if results and results.get("results"):
            context += f"\n{category.replace('_', ' ').title()} Search Results:\n"
            for result in results.get("results", []):
                content_preview = result.get('content', '')[:200]
                context += f"- {result.get('title')}: {content_preview}...\n"
        else:
            context += f"\nNo search results found for {category.replace('_', ' ').title()}\n"
    
    # Create messages for the Chat Model
    system_message = SystemMessage(content="""
    You are a music recommendation expert. Your primary rule is to ONLY suggest songs by artists that the user explicitly listed as their favorite artists in response to 'What are some of your favorite artists or bands?'. Never recommend songs by other artists, even if mentioned elsewhere in their preferences or search results.
    """)


    user_prompt = f"""
    Based ONLY on the user's stated favorite artists/bands and considering their other preferences, suggest 5-7 songs. For each song, include:
    1. Artist name (must be one of their explicitly stated favorite artists)
    2. Song title
    3. A brief explanation of why they might like it, considering their genre and mood preferences.


    User Preferences:
    {preferences}


    Potentially Relevant Search Results (for context, NOT necessarily for artists):
    {context}


    Remember: STRICTLY recommend songs ONLY by artists listed in response to 'What are some of your favorite artists or bands?'.
    """
    user_message = HumanMessage(content=user_prompt)


    # Use the LangChain ChatOpenAI instance with ainvoke
    response = chat_model.invoke([system_message, user_message])
    recommendations = response.content
    state["recommendations"] = recommendations


    # --- Prepare and Add Evaluation to State using the new helper ---
    # add_evaluation_to_state(
    #     state=state, # Pass the current state dictionary
    #     scorers=[AnswerRelevancyScorer(threshold=0.5)],
    #     input=user_prompt,
    #     actual_output=recommendations,
    #     model="gpt-4"
    # )


    judgment.async_evaluate(
        input=user_prompt,
        actual_output=recommendations,
        scorers=[AnswerRelevancyScorer(threshold=0.5)],
        model="gpt-4o"
    )
    # --- End Evaluation Setup ---


    return state


def should_continue_questions(state: State) -> bool:
    """Determine if we should continue asking questions."""
    return state["current_question_idx"] < len(state["questions"])


def router(state: State) -> str:
    """Route to the next node based on state."""
    if should_continue_questions(state):
        return "ask_question"
    return "search_music"


workflow = StateGraph(State)




workflow.add_node("ask_question", ask_question)
workflow.add_node("process_answer", process_answer)
workflow.add_node("search_music", search_music_info)
workflow.add_node("generate_recommendations", generate_recommendations)


workflow.add_edge("ask_question", "process_answer")
workflow.add_conditional_edges(
    "process_answer",
    router,
    {
        "ask_question": "ask_question",
        "search_music": "search_music"
    }
)
workflow.add_edge("search_music", "generate_recommendations")
workflow.add_edge("generate_recommendations", END)


workflow.set_entry_point("ask_question")


graph = workflow.compile()


def music_recommendation_bot(handler: JudgevalCallbackHandler, query: str):
    """Main function to run the music recommendation bot."""
    print("🎵 Welcome to the Music Recommendation Bot! 🎵")
    print("I'll ask you a few questions to understand your music taste, then suggest some songs you might enjoy.")
    print("\nRunning with predefined answers for testing...\n")
    
    # Initialize state with predefined answers
    initial_state = initialize_state()
    
    try:
        # Run the entire workflow with graph.ainvoke (asynchronous)
        # Pass handler directly in config
        # The handler instance needs to be accessible inside the node later
        config_with_callbacks = {"callbacks": [handler]}
        final_state = graph.invoke(initial_state, config=config_with_callbacks)
        
        print("\n🎧 Your Personalized Music Recommendations 🎧")
        print(final_state.get("recommendations", "No recommendations generated."))
        return final_state.get("recommendations", "No recommendations generated.")
    except Exception as e:
        print(f"An error occurred: {e}")
        return None




if __name__ == "__main__":
    handler = JudgevalCallbackHandler(judgment) 
    # music_recommendation_bot(handler)  uncomment to run without the test (if you just want tracing)
    
    # This sets us up for running the unit test
    example = Example(
        input={"handler": handler, "query": "Taylor Swift"},
        expected_tools=[
            {
                "tool_name": "search_music",
                "parameters": {
                    "query": "Taylor Swift"
                }
            }
        ]
    )
    
    client.assert_test(
        scorers=[ToolOrderScorer()],
        examples=[example],
        tracer=handler,
        function=music_recommendation_bot,
        eval_run_name="langgraph_demo",
        project_name="langgraph_demo"
    )




This below is some other file that was in the integrations folder which was using langgraph

basic.py file


from typing import Annotated, List
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage, HumanMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from judgeval.common.tracer import Tracer
from judgeval.integrations.langgraph import JudgevalCallbackHandler
import os
from dotenv import load_dotenv
from judgeval.scorers import AnswerRelevancyScorer, ExecutionOrderScorer, AnswerCorrectnessScorer
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import ToolOrderScorer


load_dotenv()


PROJECT_NAME = "LangGraphBasic"


class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]




judgment = Tracer(api_key=os.getenv("JUDGMENT_API_KEY"), project_name=PROJECT_NAME)
handler = JudgevalCallbackHandler(judgment)
client = JudgmentClient()
# REPLACE THIS WITH YOUR OWN TOOLS
def search_restaurants(location: str, cuisine: str) -> str:
    """Search for restaurants in a location with specific cuisine"""
    ans = f"Top 3 {cuisine} restaurants in {location}: 1. Le Gourmet 2. Spice Palace 3. Carbones"
    return ans


# REPLACE THIS WITH YOUR OWN TOOLS
def check_opening_hours(restaurant: str) -> str:
    """Check opening hours for a specific restaurant"""
    ans = f"{restaurant} hours: Mon-Sun 11AM-10PM"
    return ans


# REPLACE THIS WITH YOUR OWN TOOLS
def get_menu_items(restaurant: str) -> str:
    """Get popular menu items for a restaurant"""
    ans = f"{restaurant} popular dishes: 1. Chef's Special 2. Seafood Platter 3. Vegan Delight"
    example = Example(
        input="Get popular menu items for a restaurant",
        actual_output=ans
    )
    judgment.async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1)],
        example=example,
        model="gpt-4.1"
    )
    return ans 


def run_agent(prompt: str):
    tools = [
        TavilySearchResults(max_results=2),
        check_opening_hours,
        get_menu_items,
        search_restaurants,
    ]


    llm = ChatOpenAI(model="gpt-4.1")


    graph_builder = StateGraph(State)


    def assistant(state: State):
        llm_with_tools = llm.bind_tools(tools)
        response = llm_with_tools.invoke(state["messages"])
        return {"messages": [response]}


    tool_node = ToolNode(tools)
    
    graph_builder.add_node("assistant", assistant)
    graph_builder.add_node("tools", tool_node)
    
    graph_builder.set_entry_point("assistant")
    graph_builder.add_conditional_edges(
        "assistant",
        lambda state: "tools" if state["messages"][-1].tool_calls else END
    )
    graph_builder.add_edge("tools", "assistant")
    
    graph = graph_builder.compile()


    config_with_callbacks = {"callbacks": [handler]}


    result = graph.invoke({
        "messages": [HumanMessage(content=prompt)]
    }, config_with_callbacks)


    return result, handler


if __name__ == "__main__":
    current_dir = os.path.dirname(os.path.abspath(__file__))
    yaml_path = os.path.join(current_dir, "test.yaml")


    client.assert_test(
        test_file=yaml_path,
        scorers=[ToolOrderScorer()],
        function=run_agent,
        tracer=handler,
        override=True
    )


Test.yaml file

examples:
  - input:
      prompt: "Find me a good Italian restaurant in Manhattan. Check their opening hours and most popular dishes."
    expected_tools:
      - tool_name: "search_restaurants"
        parameters:
          location: "Manhattan"
          cuisine: "Italian"
      - tool_name: "check_opening_hours"
        parameters:
          restaurant: "Le Gourmet"
      - tool_name: "get_menu_items"
        parameters:
          restaurant: "Le Gourmet"


This is another folder in the same langgraph demo directory called human_in_the_loop


from typing import Annotated, List
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage, HumanMessage
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from judgeval.common.tracer import Tracer, wrap
from judgeval.integrations.langgraph import JudgevalCallbackHandler
import os
from judgeval.data import Example
from judgeval.data.datasets import EvalDataset
from judgeval.scorers import AnswerRelevancyScorer, ExecutionOrderScorer, AnswerCorrectnessScorer
from judgeval import JudgmentClient
from pydantic import BaseModel
from langgraph.types import Command, interrupt
from langgraph.checkpoint.memory import MemorySaver


PROJECT_NAME = "JNPR_MIST_LANGGRAPH"


class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]




judgment = Tracer(api_key=os.getenv("JUDGMENT_API_KEY"), project_name=PROJECT_NAME)




@judgment.observe(name="search_restaurants", span_type="tool")
def search_restaurants(location: str, cuisine: str) -> str:
    """Search for restaurants in a location with specific cuisine"""
    ans = f"Top 3 {cuisine} restaurants in {location}: 1. Le Gourmet 2. Spice Palace 3. Carbones"
    example = Example(
        input="Search for restaurants in a location with specific cuisine",
        actual_output=ans
    )
    judgment.get_current_trace().async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1)],
        example=example,
        model="gpt-4.1"
    )
    return ans


@judgment.observe(name="check_opening_hours", span_type="tool")
def check_opening_hours(restaurant: str) -> str:
    """Check opening hours for a specific restaurant"""
    ans = f"{restaurant} hours: Mon-Sun 11AM-10PM"
    example = Example(
        input="Check opening hours for a specific restaurant",
        actual_output=ans,
        expected_output=ans
    )
    judgment.get_current_trace().async_evaluate(
        scorers=[AnswerCorrectnessScorer(threshold=1)],
        example=example,
        model="gpt-4.1"
    )
    return ans


@judgment.observe(name="get_menu_items", span_type="tool")
def get_menu_items(restaurant: str) -> str:
    """Get popular menu items for a restaurant"""
    ans = f"{restaurant} popular dishes: 1. Chef's Special 2. Seafood Platter 3. Vegan Delight"
    example = Example(
        input="Get popular menu items for a restaurant",
        actual_output=ans
    )
    judgment.get_current_trace().async_evaluate(
        scorers=[AnswerRelevancyScorer(threshold=1)],
        example=example,
        model="gpt-4.1"
    )
    return ans 






@judgment.observe(name="ask_human", span_type="tool")
def ask_human(state):
    """Ask the human a question about location"""
    tool_call_id = state["messages"][-1].tool_calls[0]["id"]
    location = interrupt("Please provide your location:")
    tool_message = [{"tool_call_id": tool_call_id, "type": "tool", "content": location}]
    return {"messages": tool_message}


def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return END
    elif last_message.tool_calls[0]["name"] == "ask_human":
        return "ask_human"
    # Otherwise if there is, we continue
    else:
        return "tools"






@judgment.observe(span_type="Run Agent", overwrite=True)
def run_agent(prompt: str, follow_up_inputs: dict):
    tools = [
        TavilySearchResults(max_results=2),
        check_opening_hours,
        get_menu_items,
        search_restaurants,
    ]




    llm = ChatOpenAI(model="gpt-4.1")
    llm_with_tools = llm.bind_tools(tools + [ask_human])


    graph_builder = StateGraph(State)


    def assistant(state: State):
        response = llm_with_tools.invoke(state["messages"])
        return {"messages": [response]}


    tool_node = ToolNode(tools)
    
    graph_builder.add_node("assistant", assistant)
    graph_builder.add_node("tools", tool_node)
    graph_builder.add_node("ask_human", ask_human)


    graph_builder.set_entry_point("assistant")
    graph_builder.add_conditional_edges(
        "assistant",
        should_continue
    )
    graph_builder.add_edge("tools", "assistant")
    graph_builder.add_edge("ask_human", "assistant")
    
    checkpointer = MemorySaver()
    graph = graph_builder.compile(
        checkpointer=checkpointer
    )


    handler = JudgevalCallbackHandler(judgment)
    config = {"configurable": {"thread_id": "001"}, "callbacks": [handler]}


    for event in graph.stream(
        {
            "messages": [
                (
                    "user",
                    prompt,
                )
            ]
        },
        config,
        stream_mode="values",
    ):
        event["messages"][-1].pretty_print()


    next_node = graph.get_state(config).next
    if next_node:
        print("Resuming from checkpoint")
        print(next_node)
        input = f"{follow_up_inputs[next_node[0]]}"
        
        for event in graph.stream(Command(resume=f"{input}"), config, stream_mode="values"):
            event["messages"][-1].pretty_print()


    return handler
    


@judgment.observe(name="Test Evaluation Dataset Loop", overwrite=True)
def test_eval_dataset():
    dataset = EvalDataset()


    # Helper to configure tests with YAML
    dataset.add_from_yaml(os.path.join(os.path.dirname(__file__), "test.yaml"))
    
    for example in dataset.examples:
        print(f"Running agent for input: {example.input}")
        # Define follow-up inputs based on potential interrupt nodes
        follow_up_inputs = {"wait": "Manhattan", "ask_human": "Manhattan"}
        try:
            handler = run_agent(example.input, follow_up_inputs)
            print("Executed Node-Tools:", handler.executed_node_tools if handler else "N/A")
            example.actual_output = handler.executed_node_tools
        except Exception as e:
            print(f"Error running agent: {e}")
            logging.exception(f"Error running agent for input '{example.input}': {e}") # Suggestion: Log the full exception
            raise # Suggestion: Re-raise the exception to prevent masking it


    client = JudgmentClient()
    client.run_evaluation(
        examples=dataset.examples,
        scorers=[ExecutionOrderScorer(threshold=1, should_consider_ordering=True)],
        model="gpt-4.1",
        project_name=PROJECT_NAME,
        eval_run_name="mist-demo-examples",
        override=True
    )




if __name__ == "__main__":
    test_eval_dataset()



This is test.yaml file for it

examples:
  - input: "Find me a good Italian restaurant in this city: . Check their opening hours and also check their most popular dishes."
    follow_up_inputs:
      ask_human: "Manhattan"
    expected_output: ['assistant', 'ask_human', 'assistant', 'tools', 'tools:search_restaurants', 'assistant', 'tools', 'tools:check_opening_hours', 'assistant', 'tools', 'tools:get_menu_items', 'assistant']
  - input: "Find me a good Italian restaurant in this city: Manhattan. Check their opening hours and also check their most popular dishes."
    expected_output: ['assistant', 'tools', 'tools:search_restaurants', 'assistant', 'tools', 'tools:check_opening_hours', 'assistant', 'tools', 'tools:get_menu_items', 'assistant']
  - input: "Find me a good Italian restaurant in this city: Manhattan. Check their opening hours and also check their most popular dishes."
    expected_output: ['assistant', 'ask_human', 'tools', 'tools:search_restaurants', 'assistant', 'tools', 'tools:check_opening_hours', 'assistant', 'tools', 'tools:get_menu_items', 'assistant']
  - input: "Find me a good Italian restaurant in Manhattan."
    expected_output: ['assistant', 'tools', 'tools:search_restaurants', 'assistant']
  - input: "Check the opening hours for the restaurant called 'Le Gourmet'."
    expected_output: ['assistant', 'tools', 'tools:check_opening_hours', 'assistant']
  - input: "What are the most popular dishes at the restaurant called 'Le Gourmet'?"
    expected_output: ['assistant', 'tools', 'tools:get_menu_items', 'assistant']
  - input: "What are the most popular dishes at the restaurant called 'Le Gourmet'?"
    expected_output: ['assistant', 'tools', 'tools:search_restaurants', 'assistant']